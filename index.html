<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ying Nie </title>
    <meta name="author" content="Ying Nie">

    <!-- Le styles -->
    <link href="./my_files/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="./my_files/font-awesome.min.css">
    <link href="./my_files/style.css" rel="stylesheet" type="text/css" media="all">
    <link rel="stylesheet" id="twentytwelve-style-css" target="_blank" href="./css/style2.css" type="text/css" media="all">
    <style type="text/css">
      body {
        padding-top: 30px;
        padding-bottom: 30px;
      }

      h3 {
        margin-top: 1.0em;
        margin-bottom: 0.3em;
        padding-bottom: 0.2em;
        line-height: 1.0;
        border-bottom: 1px solid #aaaaaa;
      }

      li {
        margin: 10px 0;
      }
    </style>


  </head>

  <body>

   

<div id="wrap">

<div class="container">

  <div class="content">


<div class="row">
  <div class="span14">

<div>




<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">

<div class="post-container">
  <div class="post-thumb">
    <img src="./image/cartoon.jpg" alt="portrait" width="180" style="margin-top:5px">
    <!-- ÂõæÊ†áÈìæÊé•ÂºÄÂßã -->
    <div style="margin-top:8px; text-align:center;">
      <a href="https://github.com/nyBball" target="_blank" rel="noopener" title="GitHub">
        <i class="fa-brands fa-github" style="font-size:26px; color:#333; margin:0 6px;"></i>
      </a>
      <a href="https://scholar.google.com/citations?user=1eOYln4AAAAJ&hl=en" target="_blank" rel="noopener" title="Google Scholar">
        <i class="fa-brands fa-google-scholar" style="font-size:26px; color:#1a73e8; margin:0 6px;"></i>
      </a>
    </div>
    <!-- ÂõæÊ†áÈìæÊé•ÁªìÊùü -->
  </div>

  <div class="post-content">
    <h1 style="margin:-2px 0 0 0" class="civi_addr">Ying Nie (ËÅÇËøé)</h1>
	<p style="margin:-10px 0 0 0" class="civi_addr">
    <span style="font-style:italic; color:#ff6600;">Stay True | Passionate</span>
	</p>
	<p style="margin:3px 0 0 0" class="civi_addr">
    <p style="margin:-10px 0 0 0" class="civi_addr">Researcher, Huawei Noah's Ark Lab</p>
    <p style="margin:-10px 0 0 0" class="civi_addr">
      Email: ying.nie@zju.edu.cn, ying.nie@huawei.com
    </p>
    <p style="margin:3px 0 0 0" class="civi_addr">
      I am currently a senior researcher at Huawei Noah's Ark Lab. I received B.S. degree from the Department of Applied Mathematics at Xidian University and M.S. degree from the Department of Computer Science at Zhejiang University (advised by
      <a href="https://scholar.google.com/citations?user=jsoBob0AAAAJ&hl=en">Prof. Ruofeng Tong</a>).
      My research interests lie in efficient llm/mllm, agent and computer vision.
    </p>
  </div>
</div>

	
<div style="padding:6px;"> </div>
<H3>News</H3>
  <UL>
  <LI>
      [2025-5]üî•Our paper (<A href="https://proceedings.mlr.press/v267/han25j.html">MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles</A>) accepted to ICML 2025.  The project is available <A href="https://mor-agent.github.io/"> here</A>.
  </LI>
  <LI>
      [2025-1]üî•Our paper (<A href="https://aclanthology.org/2025.naacl-long.40/">CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models</A>) accepted to NAACL 2025 Main.  The project is available <A href="https://cfinbench.github.io/"> here</A>.
  </LI>
  </UL>


<div style="padding:6px;"> </div>
<h3>Publications</h3>
<div>
(*: Equal contribution, #: Corresponding author/Project Leader)
<p style="margin:2px 0 0 0" class="civi_addr">
<ol>

	
<b>2025</b>‚ÄÉ
  <li>
    <p>
       <a href="https://arxiv.org/abs/2505.16416">  Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models </a><br>
       Chengcheng Wang, Jianyuan Guo, Hongguang Li, Yuchuan Tian, <b>Ying Nie</b>, Chang Xu<sup>#</sup>, Kai Han<sup>#</sup><br>
       <i>arXiv,  May 2025. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://icml.cc/virtual/2025/poster/43879/">  MoRAgent: Parameter Efficient Agent Tuning with Mixture-of-Roles </a><br>
       Jing Han<sup>*</sup>, Binwei Yan<sup>*</sup>, Jianyuan Guo, Zheyuan Bai, Mengyu Zheng, Ting Han, <b>Ying Nie</b><sup>*</sup><sup>#</sup><br>
       <i>Proceedings of the 42nd International Conference on Machine Learning, ICML 2025. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://aclanthology.org/2025.naacl-long.40/">  CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models </a><br>
       <b>Ying Nie</b><sup>*</sup>, Binwei Yan<sup>*</sup>, Tianyu Guo, Hao Liu, Haoyu Wang, Wei He, Binfan Zheng, Weihao Wang, Qiang Li, Weijian Sun, Yunhe Wang<sup>#</sup>, Dacheng Tao<br>
       <i>Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL Main),  April 2025. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://ojs.aaai.org/index.php/AAAI/article/view/33206">  L-Man: A Large Multi-modal Model Unifying Human-centric Tasks </a><br>
       Jialong Zuo, <b>Ying Nie</b>, Tianyu Guo, Huaxin Zhang, Jiahao Hong, Nong Sang, Changxin Gao<sup>#</sup>, Kai Han<sup>#</sup><br>
       <i>Proceedings of the AAAI Conference on Artificial Intelligence, 2025. </i> <br>
    </p>
  </li>

	
<b>2024</b>‚ÄÉ
  <li>
    <p>
       <a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/2ce10f144bb93449767f355c01f24cc1-Abstract-Conference.html">  Cross-video identity correlating for person re-identification pre-training </a><br>
       Jialong Zuo, <b>Ying Nie</b>, Hanyu Zhou, Huaxin Zhang, Haoyu Wang, Tianyu Guo, Nong Sang, Changxin Gao<sup>#</sup><br>
       <i>Advances in Neural Information Processing Systems 37 (NeurIPS 2024) Main Conference Track. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zuo_UFineBench_Towards_Text-based_Person_Retrieval_with_Ultra-fine_Granularity_CVPR_2024_paper.html">  UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity </a><br>
       Jialong Zuo,  Hanyu Zhou, <b>Ying Nie</b>, Feng Zhang, Hanyu Zhou, Tianyu Guo, Nong Sang, Yunhe Wang, Changxin Gao<sup>#</sup><br>
       <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. </i> <br>
    </p>
  </li>


<b>2023</b>‚ÄÉ
<li>
    <p>
       <a href="https://arxiv.org/abs/2312.17276">  PanGu-œÄ: Enhancing Language Model Architectures via Nonlinearity Compensation </a><br>
       Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, <b>Ying Nie</b>, Xutao Wang, Hailin Hu, Zheyuan Bai, Yun Wang, Fangcheng Liu, Zhicheng Liu, Jianyuan Guo, Sinan Zeng, Yinchen Zhang, Qinghua Xu, Qun Liu, Jun Yao, Chao Xu, Dacheng Tao<br>
       <i>arXiv,  December 2025. </i> <br>
    </p>
</li>
<li>
    <p>
       <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/8cb92f326d01fd7f4371283ee2fa6386-Abstract-Datasets_and_Benchmarks.html">  Species196: A one-million semi-supervised dataset for fine-grained species recognition </a><br>
       Wei He, Kai Han<sup>#</sup>, <b>Ying Nie</b>, Chengcheng Wang, Yunhe Wang<sup>#</sup><br>
       <i>Advances in Neural Information Processing Systems 36 (NeurIPS 2023) Datasets and Benchmarks Track. </i> <br>
    </p>
</li>
<li>
    <p>
       <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/a0673542a242759ea637972f053b2e0b-Abstract-Conference.html">  Gold-YOLO: Efficient object detector via gather-and-distribute mechanism </a><br>
       Chengcheng Wang, Wei He, <b>Ying Nie</b>, Jianyuan Guo, Chuanjian Liu, Yunhe Wang<sup>#</sup>, Kai Han<sup>#</sup><br>
       <i>Advances in Neural Information Processing Systems 36 (NeurIPS 2023) Main Conference Track. </i> <br>
    </p>
</li>
<li>
    <p>
       <a href="https://arxiv.org/pdf/2312.00674">  Lightclip: learning multi-level interaction for lightweight vision-language models </a><br>
       <b>Ying Nie</b>, Wei He, Kai Han<sup>#</sup>, Yehui Tang, Tianyu Guo, Fanyi Du, Yunhe Wang<sup>#</sup><br>
       <i>arXiv,  December 2023. </i> <br>
    </p>
</li>


<b>2022</b>‚ÄÉ
<li>
    <p>
       <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/fcfad93e2f30ab4c22f9ec5edfbb5cc0-Abstract-Conference.html">  A transformer-based object detector with coarse-fine crossing representations </a><br>
       Zhishan Li, <b>Ying Nie</b>, Kai Han, Jianyuan Guo, Lei Xie, Yunhe Wang<sup>#</sup><br>
       <i>Advances in Neural Information Processing Systems 35 (NeurIPS 2022) Main Conference Track. </i> <br>
    </p>
</li>
<li>
    <p>
       <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/8f15e0b418ccdefec8313affc897dc8c-Abstract-Conference.html">  Redistribution of weights and activations for AdderNet quantization </a><br>
       <b>Ying Nie</b>, Kai Han, Haikang Diao, Chuanjian Liu, Enhua Wu, Yunhe Wang<sup>#</sup><br>
       <i>Advances in Neural Information Processing Systems 35 (NeurIPS 2022) Main Conference Track. </i> <br>
    </p>
</li>
<li>
    <p>
       <a href="https://openaccess.thecvf.com/content/CVPR2022W/NAS/papers/Liu_Network_Amplification_With_Efficient_MACs_Allocation_CVPRW_2022_paper.pdf">  Network amplification with efficient macs allocation </a><br>
       Chuanjian Liu, Kai Han, An Xiao, <b>Ying Nie</b>, Wei Zhang, Yunhe Wang<sup>#</sup><br>
       <i>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition WorkShop, 2022. </i> <br>
    </p>
</li>


<b>2021</b>‚ÄÉ
  <li>
    <p>
       <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/e56954b4f6347e897f954495eab16a88-Paper.pdf">  Dynamic resolution network </a><br>
       Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, <b>Ying Nie</b>, Zhenzhong Lan, Yunhe Wang<sup>#</sup><br>
       <i>Advances in Neural Information Processing Systems 34 (NeurIPS 2021). </i> <br>
    </p>
  </li>
    <li>
    <p>
       <a href="https://openreview.net/pdf?id=tbd9f3HwPy">  GhostSR: Learning Ghost Features for Efficient Image Super-Resolution </a><br>
       <b>Ying Nie</b>, Kai Han, Zhenhua Liu, Chuanjian Liu, Yunhe Wang<sup>#</sup><br>
       <i>Transactions on Machine Learning Research. </i> <br>
    </p>
  </li>
  <li>
    <p>
       <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0871.pdf">  Multi-bit Adaptive Distillation for Binary Neural Networks </a><br>
       <b>Ying Nie</b>, Kai Han, Yunhe Wang<sup>#</sup><br>
       <i>The 32nd British Machine Vision Conference (BMVC),  November 2021. </i> <br>
    </p>
  </li>

	
</ol>
</div>


<div style="padding:6px;"> </div>

<h3>Working Experience</h3>
<ul>
  <li> Researcher, Huawei Noah's Ark Lab, Beijing, China, July 2019 -  present </li>
</ul>

<div style="padding:6px;"> </div>


<div style="padding:6px;"> </div>
<h3>Academic Services</h3>
<ul>
 <li> <b>Conference Reviewer:</b> NeurIPS, ICML, ICLR, CVPR, ICCV, ECCV, AAAI, etc  </li>
	
</ul>





    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="./_files/jquery.js.download"></script>
    <script src="./my_files/bootstrap-transition.js.download"></script>
    <script src="./my_files/bootstrap-alert.js.download"></script>
    <script src="./my_files/bootstrap-modal.js.download"></script>
    <script src="./my_files/bootstrap-dropdown.js.download"></script>
    <script src="./my_files/bootstrap-scrollspy.js.download"></script>
    <script src="./my_files/bootstrap-tab.js.download"></script>
    <script src="./my_files/bootstrap-tooltip.js.download"></script>
    <script src="./my_files/bootstrap-popover.js.download"></script>
    <script src="./my_files/bootstrap-button.js.download"></script>
    <script src="./my_files/bootstrap-collapse.js.download"></script>
    <script src="./my_files/bootstrap-carousel.js.download"></script>
    <script src="./my_files/bootstrap-typeahead.js.download"></script>










</body></html>
